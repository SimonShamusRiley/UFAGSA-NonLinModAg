---
title: "Non-Linear Modeling for Dose- and Rate-Response Studies in Agriculture"
subtitle: "An Introduction in R"
author: "Simon Riley, UF|IFAS Statistical Consulting Unit"
format: 
  revealjs:
    theme: [simple, C:\Users\Simon\OneDrive - University of Florida\Workshops & Presentations\AGSA R Workshops\Nonlinear Modeling\Revealjs_Aesthetics_2023-10-10.scss]
    width: 100%
editor: source
bibliography: 'nonlinrefs.bib'
---
:::{.notes}
- Hello, everyone! For those who don't know me, my name is Simon Riley, I'm a PhD candidate in Agronomy and a data analyst with the IFAS Statistical Consulting Unit

- In today's workshop I'll be introducing the use of non-linear models for dose- and rate-response studies in agriculture, and specifically how to work with such models using the `gnls()` function from the R package `nlme`. My hope is that everyone attending will leave having learned something, but I will also say that this workshop will be most useful for those who have already had at least one graduate-level course on statistics, and have at least a basic familiarity with R.

- but before diving in, however, I have a few preliminaries to run through. First, for those of you who are not familiar with the IFAS SCU...
:::

## UF\|IFAS Statistical Consulting Unit

Open to everyone affiliated with IFAS

Provides **free assistance** with:

-   Experimental design, including sample size & power calculations

-   Data analysis, providing conceptual or applied guidance

-   Interpretation and reporting of results, including preparation of tables and figures

[Submit a consulting request at: <https://bit.ly/3Goicy9>]{.underline}

**Teams Page:** useful articles, code snippits, place to ask general stats questions.

**Seminar Series:** Basic, software agnostic introductions to methods, concepts and best practices in statistics. First Friday (usually) of every month @ 3 PM in 426 McCarty C and on Zoom. 

## Preface:

:::{.notes}
- Second, I want to briefly preface this presentation with some general guidance on why and how to use non-linear modeling

1. ...If your treatments, or one of your factors, consist of points along a continuum, we're generally more interested in understanding an overall trend than differences in the response at specific values.

2. ...Neither this nor point nor the last is an inviolable law, there are exceptions, but non-linear models are good tools to have at our disposal.

3. At the start of our analysis we often start small, fitting a model to just the data from one year, or location, or even one treatment, and this is not just OK its very prudent and can save you from much grief later on. But for our final analysis, we don't want to be fitting 8 or 10 different versions of that small model. Instead we want to build out our model to accomodate the effects of year or location or treatment and fit one model to all of our data. This greatly facilitates later testing but even if your not interested in testing between the different factors, the larger sample size provides your estimates greater precision.

4. I strongly favor training on general purpose software, rather than more user-friendly but less flexible R packages, even if it implies a somewhat steeper learning curve. We all know that the data in tutorials is nothing like data in the wild, and sooner or later you're likely to need that added flexibility. The general purpose software also often has the advantage of greater interoperability with other R packages. Note that I'm specifically refering to model fitting here - there are lots of useful packages which provide additional functionality and extend the usefulness of general purpose modeling packages like `nlme`.

5. Whatever your baseline level of frustration with R is, expect it double when starting out with nonlinear modeling. If these are methods you expect to need or use in the future, its good to get some practice in when you have time and patience. 

:::

1.  **Model Trends Using Regression**

    Researchers in agronomy frequently perform dose- or rate-response studies. Regression models, rather than ANOVA, are generally the most appropriate way to analyse these data.

::: incremental
2.  **Best Model = Good Fit to Data + Useful to Researcher (+ Justified by Theory)**

    Non-linear regression models can provide estimates of a parsimonious set of easily interpreted and biologically meaningful parameters, in a way that linear and polynomial regression sometimes cannot.

3.  **Don't Break Up Your Analysis, Build Out Your Model**

    In general, it is better to analyse the data from all treatments/years/locations together, rather than separately. This slightly complicates model specification but (1) maximizes the precision of your estimates and, (2) greatly facilitates (and improves the statistical power of) subsequent testing.
:::

## Preface:

4.  **Choose Versatile Software Tools**

    It is better to become familiar with general-purpose non-linear modeling packages (e.g., `nlme`) than to rely on more user friendly but much less flexible, domain-specific R packages.

::: incremental
5.  **Breathe**

    Fitting non-linear models requires optimization algorithms which are not guaranteed to find a solution, and the flexibility of non-linear modeling increases the scope for model misspecification/user error. Non-linear modeling thus requires more patience than when working with linear models, especially at the begining.
:::

```{r}
#| echo: false
#| message: false
#| warning: false

library(ggplot2)
library(nlme)

my_theme = theme_light()+
  theme(text = element_text(size = 14))
theme_set(my_theme)
```

## Assumptions & Scope

::: notes
-   And finally, a word on assumptions underlying what we will consider today. Statistical models can thought of as decomposing a set of observations into their deterministic component and a stochastic component

-   From here on out, we are really only concerned with the deterministic piece, and will assume the simplest and most familiar version of the stochastic piece: independent, normally distributed errors with homogeneous variance. There are tools in R to relax these assumptions and generalize to other distributions but they're beyond the scope of this tutorial.
:::

<br/> Observed Value = Deterministic Trend + Random Variation

$$
Y = f(x) + \epsilon
$$

$$
\epsilon \sim \mathrm{Normal}(0, \sigma^2)
$$

-   Henceforth, we are only concerned with $f(x)$

-   Residuals assumed to be normally distributed, independent, with uniform variance

-   Tools do exist to relax these assumptions/extend this model, but are beyond our scope

# Linear Regression Models

::: notes
-  Before considering non-linear models, we're actually going to look at linear models and how to fit them using `gnls` or "generalized nonlinear least squares" function from the `nlme` package. Although this might seem odd, it will give us the chance to learn the syntax of the `gnls()` function, as well as introduce a few concepts that will be helpful later, in a simple and familiar context.
:::

## Simple Regression {.smaller}

::: notes
-   Here we have the most commonly used formula for a simple regression model, which will be very familiar to everyone, with two parameters: $\beta_0$ known as the intercept and $\beta_1$ as the slope.

-   The intercept can be interpreted as the mean value of y when the value of x is zero, and the slope interpreted as the unit change in mean y per unit change in the x. We can see these relationships illustrated in the figure on the right.

-   In `gnls`, the model is specified almost identically to how it is written mathematically. Note that this is different from how the same model would be expressed in the the `lm` function, where only the data are given and the two parameters are implied. 

- To account for the fact that in `gnls` the model formula includes both parameters and data, there is another argument, `params`, where you define which terms in the model formula are parameters (note that all others must be the names of columns in your data.frame). In addition to identifying which terms are parameters, the params argument is also where you define *how* those parameters vary according to your treatment factors, such as the type of input or the cultivar you're applying it to. In this example, we have no such covariates, so the right hand-side of this formula is simply a 1. 

- Finally, when fitting non-linear models we need to provide starting values for each parameter in the model. When the model is fit, R will use an optimization algorithm to search, beginning from these initial values, for a set of parameter values which maximizes the likelihood function for your particular model. In all of these initial examples, I'll use starting values which match the figure shown on the right. 
:::

::: columns
::: {.column width="60%"}
Equation:

$$
f(x) = \beta_0 + \beta_1x
$$

$\beta_0$: intercept (i.e value of y when x = 0)

$\beta_1$: slope (i.e. change in y per unit change in x)

<br />

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '|2|4|5|8,9'

# R Code: 
gnls(model = y ~ b0 + b1*x, 
     data = ag_exp, 
     params = b0 + b1 ~ 1,
     start = c(b0 = 1, b1 = 0.2))

# Or, equivalently
lm(formula = y ~ x, 
   data = ag_exp)
```
:::

::: {.column width="40%"}
```{r}
#| fig-width: 6
#| fig-asp: 1

linear = function(x, b0, b1){
  b0 + b1*x
}

ggplot() +
  geom_function(fun = linear, args = list(b0 = 1, b1 = 0.2),
                col = 'steelblue', linewidth = 1.25)+
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = 1), linetype = 2, 
               arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = 0.5, y = .9, label = bquote('\u03b2'[0]), vjust = 1, size = 5)+
  scale_x_continuous(name = 'X', limits = c(0, 10), breaks = 0:10) +
  geom_segment(aes(x = 5, xend = 5, y = 2, yend = 2.2), linetype = 2)+
  geom_segment(aes(x = 5, xend = 6, y = 2.2, yend = 2.2), linetype = 2)+
  annotate('text', x = 5, y = 2.25, label = bquote('\u0394Y/\u0394X = \u03b2'[1]), vjust = 0, size = 5)+
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```
:::
:::

## Simple Regression, Reparameterized {.smaller}

:::{.notes}
- Although what we've just seen is by far the most common way of formulating a simple regression model, it is not the only way. We can, if we want, transform the x variable and/or use algebraic identities to *reparameterize* our simple regression model, with the result that the function is completely unchanged but the interprepation or meaning of our 
parameters, and correspondingly their values, do change.

- Here, I am first centering x variable around its mean, so that the intercept parameter, now denoted $\alpha_0$, is interpreted as the value of y when x is at the mean of x. We also, by defining $\alpha_1$ as the inverse of $\beta_1$ now have a rate parameter which is understood as the change in x required to produce a unit change in y.

- Fitting this parameterization requires corresponding change in the model formula and our starting values. Note that there are a few different reasons why we might want to employ a different parameterization of a model, but making it easier to choose good starting values is one of them.

:::
::: columns
::: {.column width="60%"}
Equation:

$$
f(x) = \alpha_{0} + \frac{(x - \bar{x})}{\alpha_1}
$$

$\alpha_0$: intercept (i.e value of y when x = mean(x))

$\frac{1}{\beta_1} = \alpha_1$: rate (i.e. change in x which causes 1 unit change in y)

<br />

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '|2|4|5'

# R Code: 
gnls(model = y ~ a0 + (x - mean(x))/a1, 
     data = ag_exp, 
     params = a0 + a1 ~ 1,
     start = c(a0 = 2, a1 = 5))
```
:::

::: {.column width="40%"}
```{r}
#| fig-width: 6
#| fig-asp: 1

linear = function(x, b0, b1){
  b0 + b1*x
}

ggplot() +
  geom_function(fun = linear, args = list(b0 = 1, b1 = 0.2),
                col = 'steelblue', linewidth = 1.25)+
  geom_segment(aes(x = 5, xend = 5, y = 0, yend = 2), linetype = 2,
               arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = 5.5, y = .9, label = bquote('\u03b1'[0]), vjust = 1, size = 5)+
  scale_x_continuous(name = 'X', limits = c(0, 10), breaks = 0:10) +
  geom_segment(aes(x = 3.5, xend = 3.5, y = 1.75, yend = 2.75), linetype = 2)+
  geom_segment(aes(x = 3.5, xend = 8.5, y = 2.75, yend = 2.75), linetype = 2)+
  annotate('text', x = 5, y = 2.8, label = bquote('\u0394X/\u0394Y = \u03b1'[1]), vjust = 0, size = 5)+
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```
:::
:::

## No Intercept Model, or "The One Parameter Line Function" {.smaller}

:::{.notes}
- If the last model seemed a bit contrived (although I will note without going into detail that there can be good reasons to scale and center your covariates), consider another parameterization of the linear model which does occasionally come up in agronomy: the "no-intercept model"

- This is employed when the researcher has a strong theoretical basis for believing that the average y value must be 0 when x is zero. This illustrates, firstly, how the intercept can be seen as an add-on to this more basic model, which allows the line to shift up or down, as well as how even in linear regression we're often seeking a parameterization for our model which comports with our pre-existing, practical or theoretical knowledge of the system under study.
:::

::: columns
::: {.column width="60%"}
Equation:

$$
f(x) = \beta_1x
$$

$\beta_1$: slope (i.e. change in y per unit change in x)

<br />

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '|2|4|8,9'

# R Code: 
gnls(model = y ~ b1*x, 
     data = ag_exp, 
     params = b1 ~ 1,
     start = c(b1 = 0.2))

# Or, equivalently
lm(formula = y ~ 0 + x, # or y ~ x - 1
   data = ag_exp)
```
:::

::: {.column width="40%"}
```{r}
#| fig-width: 6
#| fig-asp: 1

linear = function(x, b0, b1){
  b0 + b1*x
}

ggplot() +
  geom_function(fun = linear, args = list(b0 = 0, b1 = 0.2),
                col = 'steelblue', linewidth = 1.25)+
  scale_x_continuous(name = 'X', limits = c(0, 10), breaks = 0:10) +
  geom_segment(aes(x = 5, xend = 5, y = 1, yend = 1.2), linetype = 2)+
  geom_segment(aes(x = 5, xend = 6, y = 1.2, yend = 1.2), linetype = 2)+
  annotate('text', x = 5, y = 1.25, label = bquote('\u0394X/\u0394Y = \u03b2'[1]), vjust = 0, size = 5)+
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```
:::
:::

## ANCOVA, Common Slope {.smaller}

:::{.notes}
- We now turn to an analysis of covariance type model (still a linear regression model) but one where the slope and/or intercept can vary according to some covariate. 

- Here we consider the case of a common slope among groups, but separate intercepts. In R, our model statement is exactly the same as for the simple regression model, since we still have just an intercept and slope, but our `params` statement changes. We now need a list with two different formulas - one for each parameter), and the formula for the $\beta_0$ parameter now has a covariate (we imagine there is a factor variable named `grp` in our data frame). 

- We can also see that we need two starting values for the intercept parameter, but notice that those values are *not* equal to the actual intercepts of the two groups: the values shown in the figure correspond to coefficient values of 1 and 1 not 1 and 2, why?
:::

::: columns
::: {.column width="60%"}
Equation:

$$
f(x) = \beta_{0i} + \beta_{1}x 
$$

$\beta_0$: intercept for the *i*-th group

$\beta_{1i}$: slope for all groups

<br />

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '|4,5'

# R Code: 
gnls(model = y ~ b0 + b1*x, 
     data = ag_exp, 
     params = list(b0 ~ grp, b1 ~ 1),
     start = c(b0 = c(1, 1), b1 = 0.2))

# Or, equivalently
lm(model = y ~ grp + x, data = ag_exp)
```
:::

::: {.column width="40%"}
```{r}
#| fig-width: 6
#| fig-asp: 1

linear = function(x, b0, b1){
  b0 + b1*x
}

ggplot() +
  geom_function(fun = linear, args = list(b0 = 1, b1 = 0.2),
                col = 'steelblue', linewidth = 1.25)+
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = 1), linetype = 1, 
               arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = 0.5, y = .9, label = bquote('\u03b2'[0*', 1']), vjust = 1, size = 5)+
  geom_function(fun = linear, args = list(b0 = 2, b1 = 0.2),
                col = 'sienna', linewidth = 1.25) +
  geom_segment(aes(x = 0, xend = 0, y = 1, yend = 2), linetype = 2,
               arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = 0.5, y = 1.9, label = bquote('\u03b2'[0*', 2']), vjust = 1, size = 5)+
  annotate('text', x = 10, y = 2.75, label = 'A', color = 'steelblue', size = 6)+
  annotate('text', x = 10, y = 3.75, label = 'B', color = 'sienna', size = 6)+
  scale_x_continuous(name = 'X', limits = c(0, 10), breaks = 0:10) +
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```

:::
:::
## Interlude - Design Matrices & Contrast Specifications{.smaller} 

:::{.notes}
- To understand what's going on, we need to take a small detour to understand design matrices and contrast specifications.

- Prior to fitting a model, a design matrix is constructed which encodes our data, and in the case of factor variables, it encodes them in terms of zero/one dummy variables. 

- You can see in this very simple example here that the first column, which corresponds to the beta 0 coefficient, gets applied to all observations, while the second column, associated with the treatement effect, only gets applied to the first two observations, as the value is zero elsewhere.

- There are actually several different, somewhat arbitrary ways of constructing design matrices which depend on the type of contrast coding being employed. That choice doesn't affect the overall model, but just like with a reparameterization, it changes both the interpretation and value of the coefficients. 
:::

::: columns
::: {.column width="50%"}

- **Design matrices (X)** encode factor effects ($\mathbf{\beta_1}$) in columns of dummy variables

- There are several, somewhat arbitrary, ways to construct them depending on the **contrast coding** being used

- The choice of contrast coding thus determines interpretation of coefficients and reasonable starting values (essentially, reparameterizations of the linear model)

:::

:::{.column width="50%"}
Matrix algebra notation for a linear model:

$Y = \mathbf{X}\beta + \epsilon$

<br/>
For example, with 2 reps each of 2 treatments 
<br/>
$\begin{bmatrix}
Y_{1,1} \\ Y_{1, 2} \\ Y_{1, 1} \\ Y_{2, 2} 
\end{bmatrix}$ 
$=$
$\begin{bmatrix}
1 & 1 \\ 1 & 1 \\ 1 & 0 \\ 1 & 0 \\ 
\end{bmatrix}$
$\times$
$\begin{bmatrix}
\ \beta_0 & \beta_1  
\end{bmatrix}'$
$+$
$\begin{bmatrix}
\epsilon_{1,1} \\ \epsilon_{1,1} \\ \epsilon_{2, 1} \\ \epsilon_{2, 2} 
\end{bmatrix}$
:::
:::
## Interlude - Design Matrices & Contrast Specifications {.smaller}

:::{.notes}
- The default way that R constructs design matrices is using what it calls "treatment contrasts", which sets the value of the first factor level as the intercept, and other effects are thus the difference in value between that factor and the first factor.

- Thus if our first observation is from the "A" group, and our coefficients for the fitted model are 1 and 1, simply take the product of 1x1 + 1x0 and sum them to get the estimate of the $\beta_0$ parameter for the A group.
:::

::: columns
::: {.column width="50%"}
**"Treatment Contrasts"** (R default)

-   Intercept: value for the first factor level (a control, perhaps?)

-   Other coefficients: difference between the intercept and that factor level's value

```{r}
#| echo: false
#| eval: true
options(contrasts = c('contr.treatment', 'contr.poly'))
```

```{r}
#| echo: true
#| eval: false
# Set contrast specification
options(contrasts = c('contr.treatment', 'contr.poly'))

# print design matrix
model.matrix(~ grp, data = ag_exp)
```
<br />

```{r}
#| echo: false
trt = rep(LETTERS[1:2], each = 3)
m = model.matrix(~ trt, data.frame(trt = trt))
array(m, dim = c(6, 2), dimnames = list(grp = trt, colnames(m)))
```
:::

::: {.column width="50%"}
**ANCOVA Example:**

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '5'

# R Code: 
gnls(model = y ~ b0 + b1*x, 
     data = ag_exp, 
     params = list(b0 ~ grp, b1 ~ 1),
     start = c(b0 = c(1, 1), b1 = 2))
```

```{r}
#| fig-align: 'center'
#| fig-width: 5
#| fig-height: 4
ggplot() +
  geom_function(fun = linear, args = list(b0 = 1, b1 = 0.2),
                col = 'steelblue', linewidth = 1.25)+
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = 1), linetype = 1, 
               arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = 0.75, y = .9, label = bquote('\u03b2'[0*', 1']), vjust = 1, size = 5)+
  geom_function(fun = linear, args = list(b0 = 2, b1 = 0.2),
                col = 'sienna', linewidth = 1.25) +
  geom_segment(aes(x = 0, xend = 0, y = 1, yend = 2), linetype = 2, 
               arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = 0.75, y = 1.9, label = bquote('\u03b2'[0*', 2']), vjust = 1, size = 5)+
  annotate('text', x = 10, y = 2.75, label = 'A', color = 'steelblue', size = 6)+
  annotate('text', x = 10, y = 3.75, label = 'B', color = 'sienna', size = 6)+
  scale_x_continuous(name = 'X', limits = c(-1, 10), breaks = 0:10) +
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```
:::
:::

## Interlude - Design Matrices & Contrast Specifications {.smaller}
:::{.notes}
- An alternative, called "SAS contrasts" are basically the same but it is the last level of a factor which is treated as the reference or control.
:::

::: columns
::: {.column width="50%"}
**"SAS Contrasts"**

-   Intercept: value for the *last* factor level (a control, perhaps?)

-   Other coefficients: difference between the intercept and that factor level's value

```{r}
#| echo: false
#| eval: true
options(contrasts = c('contr.SAS', 'contr.poly'))
```

```{r}
#| echo: true
#| eval: false
# Set contrast specification
options(contrasts = c('contr.SAS', 'contr.poly'))

# print design matrix
model.matrix(~ grp, data = ag_exp)
```

<br />

```{r}
#| echo: false
m = model.matrix(~ trt, data.frame(trt = trt))
array(m, dim = c(6, 2), dimnames = list(grp = trt, colnames(m)))
```
:::

::: {.column width="50%"}
**ANCOVA Example:**

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '5'

# R Code: 
gnls(model = y ~ b0 + b1*x, 
     data = ag_exp, 
     params = list(b0 ~ grp, b1 ~ 1),
     start = c(b0 = c(2, -1), b1 = 2))
```

```{r}
#| fig-align: 'center'
#| fig-width: 5
#| fig-height: 4
ggplot() +
  geom_function(fun = linear, args = list(b0 = 1, b1 = 0.2),
                col = 'steelblue', linewidth = 1.25)+
  geom_segment(aes(x = -.1, xend = -.1, y = 0, yend = 2), linetype = 1, 
               arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = -0.75, y = 2.2, label = bquote('\u03b2'[0*', 1']), vjust = 1, size = 5)+
  geom_function(fun = linear, args = list(b0 = 2, b1 = 0.2),
                col = 'sienna', linewidth = 1.25) +
  geom_segment(aes(x = .1, xend = .1, y = 2, yend = 1), linetype = 2, 
               arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = 0.75, y = 0.95, label = bquote('\u03b2'[0*', 2']), vjust = 1, size = 5)+
  annotate('text', x = 10, y = 2.75, label = 'A', color = 'steelblue', size = 6)+
  annotate('text', x = 10, y = 3.75, label = 'B', color = 'Sienna', size = 6)+
  scale_x_continuous(name = 'X', limits = c(-1, 10), breaks = 0:10) +
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```
:::
:::

## Interlude - Design Matrices & Contrast Specifications {.smaller}

:::{.notes}
- There are also "sum-to-zero" contrasts, where the last factor level is set equal to -1 times the sum of the other coefficients, or the difference in that factor level and the overall average level among all factor levels. 
:::

::: columns
::: {.column width="50%"}
**"Sum-to-Zero Contrasts"**

-   Intercept: average value across all factor levels

-   Other coefficients: difference between the intercept and that factor level's value


```{r}
#| echo: false
#| eval: true
options(contrasts = c('contr.sum', 'contr.poly'))
```

```{r}
#| echo: true
#| eval: false
# Set contrast specification
options(contrasts = c('contr.sum', 'contr.poly'))

# print design matrix
model.matrix(~ grp, data = ag_exp)
```

<br />

```{r}
#| echo: false
m = model.matrix(~ trt, data.frame(trt = trt))
array(m, dim = c(6, 2), dimnames = list(grp = trt, colnames(m)))
```
:::

::: {.column width="50%"}
**ANCOVA Example:**

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '5'

# R Code: 
gnls(model = y ~ b0 + b1*x, 
     data = ag_exp, 
     params = list(b0 ~ grp, b1 ~ 1),
     start = c(b0 = c(2.5, -0.5), b1 = 2))
```

```{r}
#| fig-align: 'center'
#| fig-width: 5
#| fig-height: 4
ggplot() +
  geom_function(fun = linear, args = list(b0 = 1, b1 = 0.2),
                col = 'steelblue', linewidth = 1.25)+
  annotate('point', x = 0, y = 1.5, size = 2)+
  geom_segment(aes(x = -.1, xend = -.1, y = 0, yend = 1.5),
               linetype = 1, arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = -0.75, y = 1.5, label = bquote('\u03b2'[0*', 1']), vjust = 1, size = 5)+
  geom_function(fun = linear, args = list(b0 = 2, b1 = 0.2),
                col = 'sienna', linewidth = 1.25) +
  geom_segment(aes(x = .1, xend = .1, y = 1.5, yend = 1), linetype = 2,
               arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = 0.75, y = 0.95, label = bquote('\u03b2'[0*', 2']), vjust = 1, size = 5)+
  geom_segment(aes(x = 0, xend = 0, y = 1.5, yend = 2), linetype = 2,
               arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = 0.75, y = 1.95, label = bquote('-\u03b2'[0*', 2']), vjust = 1, size = 5)+
  annotate('text', x = 10, y = 2.75, label = 'A', color = 'steelblue', size = 6)+
  annotate('text', x = 10, y = 3.75, label = 'B', color = 'sienna', size = 6)+
  scale_x_continuous(name = 'X', limits = c(-1, 10), breaks = 0:10) +
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```
:::
:::

## Interlude - Design Matrices & Contrast Specifications {.smaller}

:::{.notes}
- Each of these contrast specifications have different instances where they are more or less useful, but in terms of setting starting values, I find it extremely useful to use the "no intercept" trick, which by removing the intercept from the design matrix entirely, allows each column to correspond to one of the groups. This works regardless of which contrast specification is used.
:::

::: columns
::: {.column width="50%"}
**"No Intercept"**

-   Each coefficient is the value for that group


```{r}
#| echo: false
#| eval: true
options(contrasts = c('contr.treatment', 'contr.poly'))
```

```{r}
#| echo: true
#| eval: false
# This works for any contrast specification, but becomes slightly
# complicated in the presence of interactions

# print design matrix
model.matrix(~ 0 + grp, data = ag_exp)
```
<br />

```{r}
#| echo: false
m = model.matrix(~ 0+trt, data.frame(trt = trt))
array(m, dim = c(6, 2), dimnames = list(grp = trt, colnames(m)))
```
:::

::: {.column width="50%"}
**ANCOVA Example:**

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '4,5'
# R Code: 
gnls(model = y ~ b0 + b1*x, 
     data = ag_exp, 
     params = list(b0 ~ 0 + grp, b1 ~ 1),
     start = c(b0 = c(1, 2), b1 = 2))
```

```{r}
#| fig-align: 'center'
#| fig-width: 5
#| fig-height: 4
ggplot() +
  geom_function(fun = linear, args = list(b0 = 1, b1 = 0.2),
                col = 'steelblue', linewidth = 1.25)+
  geom_segment(aes(x = -.1, xend = -.1, y = 0, yend = 1),
               linetype = 1, arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = -0.75, y = 0.5, label = bquote('\u03b2'[0*', 1']), vjust = 0.5, size = 5)+
  geom_function(fun = linear, args = list(b0 = 2, b1 = 0.2),
                col = 'sienna', linewidth = 1.25) +
  geom_segment(aes(x = .1, xend = .1, y = 0, yend = 2),
               linetype = 2, arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  annotate('text', x = 0.75, y = 1.5, label = bquote('\u03b2'[0*', 2']), vjust = .5, size = 5)+
  annotate('text', x = 10, y = 2.75, label = 'A', color = 'steelblue', size = 6)+
  annotate('text', x = 10, y = 3.75, label = 'B', color = 'sienna', size = 6)+
  scale_x_continuous(name = 'X', limits = c(-1, 10), breaks = 0:10) +
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```
:::
:::

## ANCOVA, Separate Intercepts & Slopes {.smaller}

:::{.notes}
- And generalizing all of this to an ancova type model with seperate slopes and intercepts is straightforward, we simply revert back to a common params formula, as both the intercept and slope vary by group, and provide two starting values for each of the two parameters.
:::

::: columns
::: {.column width="60%"}
Equation:

$$
f(x) = \beta_{0i} + \beta_{1i}x 
$$

$\beta_0$: intercept for the *i*-th group

$\beta_{1i}$: slope for the *i*-th group

<br />

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '|2|4,5'

# R Code: 
gnls(model = y ~ b0 + b1*x, 
     data = ag_exp, 
     params = b0 + b1 ~ 0 + group,
     start = c(b0 = c(1.5, 1.0), b1 = c(0.125, 0.2)))

# Or, equivalently
lm(model = y ~ 0 + grp + grp:x, data = ag_exp) 
```
:::

::: {.column width="40%"}
```{r}
#| fig-width: 6
#| fig-asp: 1

linear = function(x, b0, b1){
  b0 + b1*x
}

ggplot() +
  geom_function(fun = linear, args = list(b0 = 1.5, b1 = 0.125),
                col = 'steelblue', linewidth = 1.25)+
  #geom_segment(aes(x = 0, xend = 0, y = 0, yend = 1), linetype = 1, 
  #             arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  #annotate('text', x = 0.5, y = .9, label = bquote('\u03b2'[0*', 1']), vjust = 1, size = 5)+
  geom_function(fun = linear, args = list(b0 = 1, b1 = 0.2),
                col = 'sienna', linewidth = 1.25) +
  #geom_segment(aes(x = 0, xend = 0, y = 1, yend = 2), linetype = 2,
  #             arrow = grid::arrow(ends = 'last', length = unit(.25, 'cm'))) +
  #annotate('text', x = 0.5, y = 1.9, label = bquote('\u03b2'[0*', 2']), vjust = 1, size = 5)+
  scale_x_continuous(name = 'X', limits = c(0, 10), breaks = 0:10) +
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```
:::
:::

## Polynomial Regression {.smaller}

:::{.notes}
-Not all trends exhibit curvature, and a commonly employed way to account for this is polynomial regression, such as the following quadratic regression model.

- This model is easy to fit using `lm` or other linear modeling functions, but terms are not all easy to interpret. The intercept has the same meaning as in the other regression models, but the "slope" is now only actually the slope when x is exactly 0, and the $\beta_2$ perameter is the rate of change in the rate of change, or decelleration.
:::

::: columns
::: {.column width="60%"}
Equation:

$$
f(x) = \beta_{0} + \beta_{1}x + \beta_2x^2 
$$

$\beta_0$: intercept

$\beta_{1}$: slope (when x is 0?)

$\beta_2$: "deceleration" (?)

<br />

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '|'

# R Code: 
gnls(model = y ~ b0 + b1*x + b2*x^2, 
     data = ag_exp, 
     params = b0 + b1 + b2~ 1,
     start = c(b0 = 1.0, b1 = 0.2, b2 = -0.015))

# Or, equivalently
lm(model = y ~ x + I(x^2), data = ag_exp) 
```
:::

::: {.column width="40%"}
```{r}
#| fig-width: 6
#| fig-asp: 1

quad = function(x, b0, b1, b2){
  b0 + b1*x + b2*x^2
}

ggplot() +
  geom_function(fun = quad, args = list(b0 = 1, b1 = 0.2, b2 = -0.02),
                col = 'steelblue', linewidth = 1.25)+
  scale_x_continuous(name = 'X', limits = c(0, 10), breaks = 0:10) +
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```
:::
:::

## Polynomial Regression, Reparameterized {.smaller}

:::{.notes}
- But, we actually have the option to instead reparameterize this, expressing it using the formula commonly used parabolas (which this curve is) in geometry, with the advantage that our parameters are now much more easily interpreted.

- This parameterization cannot be fit using `lm` however, because it is in fact, non-linear
:::
::: columns
::: {.column width="60%"}
Equation:

$$
f(x) = \alpha_1 + \kappa(x - \alpha_0)^2
$$

$\alpha_0$: optimum x value (i.e. x value which maximizes y)

$\alpha_{1}$: maximum y value

$\kappa$: related to "steepness" of the curve

<br />

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '|'

# R Code: 
gnls(model = y ~ a1 + k1*(x - a0)^2, 
     data = ag_exp, 
     params = a0 + a1 + k ~ 1,
     start = c(a0 = 5, a1 = 1.5, k = -0.02))
```
:::

::: {.column width="40%"}
```{r}
#| fig-width: 6
#| fig-asp: 1

parabola = function(x, a0, a1, k){
  a1 + k*(x - a0)^2
}

ggplot() +
  geom_function(fun = parabola, args = list(a0 = 5, a1 = 1.5, k = -1/50),
                col = 'steelblue', linewidth = 1.25)+
  geom_segment(aes(x = 0, xend = 5, y = 0, yend = 0), 
               arrow = grid::arrow(ends = 'last', length = unit(0.5, 'cm')), 
               linetype = 2)+
  annotate('text', x = 4.5, y = .125, label = bquote('\u03b1'[0]), size = 5)+
  geom_segment(aes(x = 5, xend = 5, y = 0, yend = 1.5), 
               arrow = grid::arrow(ends = 'last', length = unit(0.5, 'cm')), 
               linetype = 2)+
  annotate('text', x = 5.5, y = 1.25, label = bquote('\u03b1'[1]), size = 5)+
  scale_x_continuous(name = 'X', limits = c(0, 10), breaks = 0:10) +
  scale_y_continuous(name = 'Y', limits = c(0, 4))
```
:::
:::

# Non-Linear Regression Models

:::{.notes}
- I've given the technical definition of a non-linear model there but what matters is that it actually has nothing to do with the curviness of the function, and sometimes even the same function can be linear or nonlinear depending on the paramterization

- Instead, you can think of nonlinear models as those which...

- In the following slides, we'll only consider a couple of them but there are many, many models which have been developed for different systems or biological phenomena, often with numerous different parameterizations, and at the end I'll provide references which provide much more detailed descriptions of many of them.
:::

<br/> **Technical Definition**: Functions with a first derivative with respect to at least one parameter which is a function of another parameter (takeaway is that its not actually about "curviness", and some functions can be linear or nonlinear depending on the parameterization).

<br/> **Plain(-ish) Language Summary**: Functions which exhibit some combination of *limiting behavior*, such as asymptotically approaching an upper or lower limit, *critical points*, such as a maximum or minimum, and *inflection points*, where a trend switches from accelerating to decelerating.

## Michaelis-Menton Function {.smaller}

:::{.notes}
- The first that we'll consider is the Michaelis-Menton function equation, otherwise known as the Monad function, Shinozaki-Kira function, and possibly others. 

- The alpha parameter...

:::

::: columns
::: {.column width="60%"}
Equation:

$$
f(x) = \frac{\alpha x}{\beta + x}
$$

$\alpha$: asymptote (y value as $x \rightarrow \infty$)

$\beta$: half-maximum (x value where y = $\frac{\alpha}{2}$)

<br />

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: '|'

# R Code: 
gnls(model = y ~ a*x/(b+x), 
     data = ag_exp, 
     params = a + d ~ 1,
     start = c(a = 3, b = 1.7))
```
:::

::: {.column width="40%"}
```{r}
#| fig-width: 6
#| fig-asp: 1

michmen = function(x, a, b){
  a*x/(b+x)
}

ggplot() +
  geom_function(fun = michmen, args = list(a = 3, b = 1.7),
                col = 'steelblue', linewidth = 1.25)+
  geom_hline(yintercept = 3, linetype = 2)+
  annotate('text', x = 20, y = 3.1, label = bquote('\u03b1'), size = 5) +
  geom_segment(aes(x = 1.7, xend = 1.7, y = 0, yend = 1.5), 
               arrow = grid::arrow(ends = 'last', length = unit(0.5, 'cm')), 
               linetype = 2)+
  annotate('text', x = 2.5, y = 0.1, label = bquote('\u03b2'), size = 5) +
  geom_segment(aes(x = 0, xend = 1.7, y = 0, yend = 0), 
               arrow = grid::arrow(ends = 'last', length = unit(0.5, 'cm')), 
               linetype = 2) +
  annotate('text', x = 2.5, y = 1.5, label = bquote('\u03B1/2'), size = 5) +
  scale_x_continuous(name = 'X', limits = c(0, 20), breaks = 0:10*2) +
  scale_y_continuous(name = 'Y', limits = c(0, 3.5))
```
:::
:::

## Michaelis-Menton Function {.smaller}

:::{.notes}
- Although our fitted model gives us the maximum possible response, we're often interested in a target value which achieves some percentage of that maximum, which can be calculated using this formula here. 

- Alternatively, if we have price information we can calculate, by plugging in our model estimates into this formula, the net return, which using this formula allows us to calculate an economic optimum rate.

- I'm glossing over where these formula come from because that's less imporant for you to know than that you have an appreciation for the kinds of values that can be derived from this and other, similar, models. But with that said, most of this can be derived just using very basic algebra.  

:::

::: columns
::: {.column width="60%"}
Target rate, $T_p$, will be x value that achieves some percent, *P*, of the asymptote:

$$
T_p = \frac{\beta P}{1-P}
$$

or, if prices, $C_x$ and $C_y$, are known for x and y, respectively, then you can calculate net returns ($I_{net}$):

$$
I_{net} = \frac{C_y\alpha x}{\beta + x}  - xC_x 
$$
$\alpha$: asymptote (y value as $x \rightarrow \infty$)

$\beta$: half-maximum (x value where y = $\frac{\alpha}{2}$)

:::

::: {.column width="40%"}
```{r}
#| fig-width: 6
#| fig-asp: 1

michmen = function(x, a, b){
  a*x/(b+x)
}

net = function(x, a, b, cx, cy){
  (cy*a*x/(b+x) - cx*x)
}

ggplot() +
  geom_function(fun = michmen, args = list(a = 3, b = 1.7),
                col = 'steelblue', linewidth = 1.25)+
  geom_hline(yintercept = 3, linetype = 2)+
  annotate('text', x = 20, y = 3.1, label = bquote('\u03b1'), size = 5) +
  geom_segment(aes(x = 1.7, xend = 1.7, y = 0, yend = 1.5), 
               arrow = grid::arrow(ends = 'last', length = unit(0.5, 'cm')), 
               linetype = 2)+
  annotate('text', x = 2.5, y = 0.1, label = bquote('\u03b2'), size = 5) +
  geom_segment(aes(x = 0, xend = 1.7, y = 0, yend = 0), 
               arrow = grid::arrow(ends = 'last', length = unit(0.5, 'cm')), 
               linetype = 2) +
  annotate('text', x = 2.75, y = 1.5, label = bquote('\u03B1/2'), size = 5) +
  
  geom_segment(aes(x = 0, xend = 15.3, y = -0.05, yend = -0.05), 
               arrow = grid::arrow(ends = 'last', length = unit(0.5, 'cm')), 
               linetype = 2)+
  annotate('text', x = 16.5, y = 0.1, label = bquote('T'['90%']), size = 5) +
  geom_segment(aes(x = 15.3, xend = 15.3, y = 0, yend = 2.7), 
               arrow = grid::arrow(ends = 'last', length = unit(0.5, 'cm')), 
               linetype = 2)+
  annotate('text', x = 17, y = 2.5, label = bquote('\u03b1*90%'), size = 5) +
  
  geom_function(fun = net, args = c(a = 3, b = 1.7, cx = .05, cy = 1.0), 
                color = 'sienna', linewidth = 1.25)+
  scale_x_continuous(name = 'X', limits = c(0, 20), breaks = 0:10*2) +
  scale_y_continuous(name = 'Y', limits = c(-0.1, 3.5),
                     sec.axis = sec_axis(name = bquote('I'[`net`]),
                                         trans = ~ 12 + .*10)) +
  theme(axis.title.y.left = element_text(color = 'steelblue', face = 'bold'),
        axis.title.y.right = element_text(color = 'sienna', face = 'bold'))
```
:::
:::


## Example 1: Onion Yield Response to Seeding Rate {.smaller}

First, run the following (after installing the required packages, as necessary) to set up our R environment:

```{r}
#| echo: true
#| warning: false
#| messages: false

# Load packages
library(agridat)
library(tidyverse)
library(nlme)
library(ggResidpanel)
library(emmeans)
library(marginaleffects)
library(car)

# set ggplot aesthetics
my_pal = c('steelblue', 'sienna', 'olivedrab', 'goldenrod')
my_theme = theme_classic()+
  theme(text = element_text(size = 12), 
        legend.position = 'bottom')

options(ggplot2.discrete.colour = my_pal, 
        ggplot2.discrete.fill = my_pal)
theme_set(my_theme)
```


## Example 1: Onion Yield Response to Seeding Rate {.smaller}

This data is adapted from @schabenbergerContemporaryStatisticalModels2001 and originally reported in @meadPlantDensityCrop1970. It contains the yield ($\mathrm{kg/m^2}$) of three onion (*Allium cipa* L.) cultivars when planted at various densities ($\mathrm{plants/m^2}$). @schabenbergerContemporaryStatisticalModels2001 fit an alternative parameterization of the Michaelis-Menton function and referred to it by a different name.

```{r}
#| echo: true
#| eval: true
#| output-location: 'column'

# Load and prep data
onion = data.frame(
   Cultivar = factor(rep(LETTERS[1:3], each = 10)), 
   Density = c(33.045, 35.629, 64.261, 75.24, 93.323,
               144.129, 192.243, 232.178, 309.678, 334.542,
               23.035, 28.524, 40.903, 56.403, 84.281,
               93.861, 108.823, 173.084, 228.41, 276.74, 
               26.694, 37.997, 47.899, 67.059, 88.587,
               103.226, 181.587, 201.177, 277.063, 326.469), 
   Yield = c(3.49, 3.185, 4.562, 4.537, 4.442, 
             5.434, 5.825, 5.619, 6.441, 6.189,
             3.031, 3.112, 3.833, 4.072, 4.475, 
             4.665, 4.114, 5.764, 5.596, 5.064,
             3.118, 3.48, 3.482, 3.541, 4.323,
             4.036, 5.502, 4.868, 5.541, 5.321))

str(onion, width = 60, strict.width = 'cut')
```


## Exploratory Data Analysis (Ex. 1){.smaller}

```{r}
#| echo: true
#| eval: true
#| fig-width: 15 
#| fig-asp: .33
#| fig-align: 'center'
#| output-location: 'fragment'

ggplot(onion, aes(x = Density, y = Yield, color = Cultivar))+
  # Plot the data itself
  facet_wrap(~Cultivar)+ 
  geom_point(size = 3)
```


## Exploratory Data Analysis (Ex. 1){.smaller}

```{r}
#| echo: true
#| eval: true
#| code-line-numbers: '|5-6'
#| output-location: 'fragment'
#| fig-width: 15
#| fig-asp: .33
#| fig-align: 'center'
ggplot(onion, aes(x = Density, y = Yield, color = Cultivar))+
  # Plot the data itself 
  facet_grid(cols = vars(Cultivar))+ 
  geom_point(size = 3) +
  # Plot linear trend
  geom_smooth(method = 'lm', se = F, size = .5)
```

## Exploratory Data Analysis (Ex. 1){.smaller}
:::columns
:::{.column width="50%"}

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: '|1-4|6-8|14-18'
# Michaelis-Menton function
micmen = function(x, ymax, xhalf){
  ymax*x/(xhalf + x)
}

# Considering just cultivar A
onion |> 
filter(Cultivar == 'A') |> 
ggplot(aes(x = Density, y = Yield, color = Cultivar))+
  # Plot the data itself 
  geom_point(size = 3) +
  # Plot linear trend
  geom_smooth(method = 'lm', se = F, size = .5) +
  # Plot Mitscherlich trend, using this 
  # to identify reasonable starting values
  geom_function(fun = micmen, 
                args = list(ymax = 7,
                            xhalf = 40), 
                size = 0.5, linetype = 2)
```
:::

:::{.column width="50%"}
```{r}
#| echo: false
#| eval: true
#| code-line-numbers: '|1-4|6-8|14-18'
#| fig-width: 7
#| fig-asp: 1

# Michaelis-Menton function
micmen = function(x, ymax, xhalf){
  ymax*x/(xhalf + x)
}

# Considering just cultivar A
onion |> 
filter(Cultivar == 'A') |> 
ggplot(aes(x = Density, y = Yield, color = Cultivar))+
  # Plot the data itself 
  geom_point(size = 3) +
  # Plot linear trend
  geom_smooth(method = 'lm', se = F, size = .5) +
  # Plot Mitscherlich trend, using this 
  # to identify reasonable starting values
  geom_function(fun = micmen, 
                args = list(ymax = 7,
                            xhalf = 40), 
                size = 0.5, linetype = 2)
```
:::

:::

## Model Development & Evaluation (Ex. 1){.smaller}

First, fit the linear model (this could be done using `lm`).

```{r}
#| echo: true
#| eval: true
#| output-location: default
#| fig-width: 6
#| fig-asp: 1
#| fig-align: 'center'
#| code-line-numbers: '|2|4|5-6|7'
# Fit the linear (ANCOVA) regression model
onion_mod_lin = gnls(model = Yield ~ yinit + slope*Density,
                    data = onion, 
                    params = yinit + slope ~ 0 + Cultivar, 
                    start = c(yinit = c(3.5, 3.5, 3.5), 
                              slope = c(0.01, 0.01, 0.01)))
print(onion_mod_lin)
```

## Model Development & Evaluation (Ex. 1){.smaller}

Next, fit the Mitscherlich model:

```{r}
#| echo: true
#| eval: true
#| output-location: default
#| fig-width: 6
#| fig-asp: 1
#| fig-align: 'center'
#| code-line-numbers: '|2|4|5-7|8'
# Fit the Michaelis-Menton model
onion_mod_mm = gnls(model = Yield ~ ymax*Density/(xhalf + Density), 
                     data = onion, 
                     params = ymax + xhalf ~ 0 + Cultivar, 
                     start = c(curv = c(40, 40, 40),
                               ymax = c(7, 7, 7)))
print(onion_mod_mm)
```

## Model Development & Evaluation (Ex. 1){.smaller}

$\Delta$AIC strongly favors of the non-linear model. The diagnostic plots of the residuals indicate possible minor violations of the assumptions of normality and homoscedasticity, which in other circumstances I would probe into further, but that is beyond the scope of this workshop.

:::columns

:::{.column width="50%"}

```{r}
#| echo: true
#| eval: true
# compare the two models
AIC(onion_mod_lin, onion_mod_mm)
```

<br/>

```{r}
#| echo: true
#| eval: false
onion_mod_res = resid(onion_mod_mm, type = 'normalized')
onion_mod_pred = fitted(onion_mod_mm)
resid_auxpanel(onion_mod_res, onion_mod_pred)
```
:::

:::{.column width="50%"}

```{r}
#| echo: false
#| fig-width: 6
#| fig-asp: 1
#| fig-align: 'center'
onion_mod_res = resid(onion_mod_mm, type = 'normalized')
onion_mod_pred = fitted(onion_mod_mm)
resid_auxpanel(onion_mod_res, onion_mod_pred)
```

:::

:::

## Estimation, Inference and Graphing (Ex. 1){.smaller}

All the tools from the `emmeans` package will work with models fit with `gnls`, you simply need to specify the parameter of interest via the argument `param`:

```{r}
#| echo: true
#| eval: true
#| output-location: 'column-fragment'
# Perform F-Test
joint_tests(onion_mod_mm, param = 'ymax')
```
<br/>

```{r}
#| echo: true
#| eval: true
#| output-location: 'column-fragment'
# Calculate estimated marginal means
(onion_emm_ymax = emmeans(onion_mod_mm, ~ Cultivar,
                         param = 'ymax'))
```
<br/>

```{r}
#| echo: true
#| eval: true
#| output-location: 'column-fragment'
# Perform contrasts 
contrast(onion_emm_ymax, 'pairwise')
```

## Estimation, Inference and Graphing (Ex. 1){.smaller}

For derived quantities such as $T_{90\%}$, we can use the delta method to generate estimates, standard errors, and confidence intervals:

```{r}
#| echo: true
#| eval: true
#| output-location: 'column-fragment'
# names of coefficients used in formula for 
# derived quantities must match output from coef()
print(names(coef(onion_mod_mm)), width = 40)
```
<br/>
```{r}
#| echo: true
#| eval: true
#| output-location: 'column-fragment'
#| code-line-numbers: '|3|4|5|14'
# Calculate 90% target rate for each cultivar, and 
# output the results
t90a = deltaMethod(object = onion_mod_mm, 
                   g. = 'xhalf.CultivarA*0.9/(1 - 0.9)', 
                   func = 'T90% for Cultivar A:')

t90b = deltaMethod(object = onion_mod_mm, 
                   g. = 'xhalf.CultivarB*0.9/(1 - 0.9)', 
                   func = 'T90% for Cultivar B:')

t90c = deltaMethod(object = onion_mod_mm, 
                   g. = 'xhalf.CultivarC*0.9/(1 - 0.9)', 
                   func = 'T90% for Cultivar C:')
rbind(t90a, t90b, t90c)
```

## Estimation, Inference and Graphing (Ex. 1){.smaller}

The `marginaleffects` package has additional tools for generating estimates and confidence band for the fitted trendline:

```{r}
#| echo: true
#| eval: true
#| fig-width: 15
#| fig-asp: 0.33
#| fig-align: 'center'
#| output-location: 'slide'
#| code-line-numbers: '|1-4|6-10|12-18'
# Generate a new data set containing all combinations of 
# cultivar levels and a fine grid of density values
toplot = expand.grid(Cultivar = factor(LETTERS[1:3]),
                     Density = 0:33*10) 

# Generate predictions for these new cultivar and density
# combinations
onion_preds = avg_predictions(model = onion_mod_mm, 
                             newdata = toplot, 
                             by = c('Cultivar', 'Density'))

# Plot the results, along with the original data
ggplot(onion_preds, aes(x = Density, color = Cultivar, fill = Cultivar)) +
  facet_grid(cols = vars(Cultivar))+
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high),
              color = NA, alpha = 0.2) +
  geom_line(aes(y = estimate))+
  geom_point(aes(y = Yield), data = onion, size = 2)
```



## Log-Logistic Function{.smaller}

::: columns
:::{.column width="60%"}
Equation:

$$
f(x) = \alpha_0 + \frac{\alpha_1 - \alpha_0}{1+e^{-\beta(\mathrm{ln}(x)-\mathrm{ln}(EC_{50}))}}
$$

$\alpha_0$ is the intercept

$\alpha_1$ is the lower limit

$\beta$ is the "slope"

$EC_{50}$ is half-maximal effective concentration

<br/>

```{r}
#| echo: true
#| eval: false
# R Code:
gnls(y ~ a0 + (a1 - a0)/(1+exp(-b*(log(x) - log(EC50)))),  
     data = ag_exp, 
     params = a0 + a1 + b + IC50 ~ 1, 
     start = c(a0 = 1, a1 = 0.2, b = 1, EC50 = 3))
```

:::

:::{.column width="40%"}
```{r}
#| eval: true
#| echo: false
#| fig-width: 6
#| fig-asp: 1

loglogistic = function(x, a0, a1, b, c){
  a0+(a1-a0)/(1+exp(-b*(log(x)-log(c))))
}
ggplot()+
  geom_function(fun = loglogistic, 
                args = list(a0 = 1, a1 = 0.2, b = 1, c = 3), 
                color = 'steelblue', linewidth = 1.25)+
  scale_x_continuous(name = 'X', limits = c(0, 20), breaks = 0:10*2)+
  scale_y_continuous(name = 'Y', limits = c(0, 1)) +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = 1),
               arrow = grid::arrow(length = unit(5, 'mm')), 
               linetype = 2) +
  annotate('text', x = 1, y = 1, label = bquote('\u03b1'[0]), size = 5) +
  
  geom_segment(aes(x = 0, xend = 3, y = 0, yend = 0),
               arrow = grid::arrow(length = unit(5, 'mm')), 
               linetype = 2) +
  annotate('text', x = 4.5, y = 0, label = bquote('EC'[50]), size = 5) +
  
  geom_segment(aes(x = 3, xend = 3, y = 0, yend = 0.6), 
               arrow = grid::arrow(length = unit(5, 'mm')),
               linetype = 2) +
  annotate('text', x = 7, y = 0.6, label = bquote(a[0] - (a[1]-a[0])/2), size = 5) +
  
  geom_hline(yintercept = 0.2, linetype = 2) +
  annotate('text', x = 20, y = 0.225, label = bquote('\u03b1'[1]), size = 5)
```

:::
:::

## Exponential Decay Function{.smaller}

::: columns
:::{.column width="60%"}
Equation:

$$
f(x) = \alpha_1 + (\alpha_0 - \alpha_1)e^{-\frac{x}{\beta}}
$$

$\alpha_0$ is the intercept

$\alpha_1$ is the lower limit

$\beta$ is the "slope"

<br/>

```{r}
#| echo: true
#| eval: false
# R Code:
gnls(y ~ a1+(a0-a1)*exp(-x/b),  
     data = ag_exp, 
     params = a0 + a1 + b ~ 1, 
     start = c(a0 = 1, a1 = 0.2, b = 4.5))
```

:::

:::{.column width="40%"}
```{r}
#| eval: true
#| echo: false
#| fig-width: 6
#| fig-asp: 1

expon = function(x, a0, a1, b){
     a1+(a0-a1)*exp(-x/b)
  }
ggplot()+
  geom_function(fun = loglogistic, 
                args = list(a0 = 1, a1 = 0.2, b = 1, c = 3), 
                color = 'steelblue', linewidth = 1.25, alpha = 0.2)+
  geom_function(fun = expon, 
                args = list(a0 = 1, a1 = 0.2, b = 4.5), 
                color = 'sienna', linewidth = 1.25)+
  scale_x_continuous(name = 'X', limits = c(0, 20), breaks = 0:10*2)+
  scale_y_continuous(name = 'Y', limits = c(0, 1)) +
  
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = 1),
               arrow = grid::arrow(length = unit(5, 'mm')), 
               linetype = 2) +
  annotate('text', x = 1, y = 1, label = bquote('\u03b1'[0]), size = 5) +
  
  geom_hline(yintercept = 0.2, linetype = 2) +
  annotate('text', x = 20, y = 0.225, label = bquote('\u03b1'[1]), size = 5)
```

:::
:::

## Hyperbolic Function{.smaller}

::: columns
:::{.column width="60%"}
Equation:

$$
f(x) = \alpha_1 + \frac{\beta*(\alpha_0 - \alpha_1)}{(\beta+x)}
$$

$\alpha_0$ is the intercept

$\alpha_1$ is the lower limit

$\beta$ is the "slope"


<br/>

```{r}
#| echo: true
#| eval: false
# R Code:
gnls(y ~ a1 + b*(a0-a1)/(b+x),  
     data = ag_exp, 
     params = a0 + a1 + b ~ 1, 
     start = c(a0 = 1, a1 = 0.2, b = 1, b = 2))
```

:::

:::{.column width="40%"}
```{r}
#| eval: true
#| echo: false
#| fig-width: 6
#| fig-asp: 1

hyperbolic = function(x, a0, a1, b){
  a1 + b*(a0-a1)/(b+x)
}
ggplot()+
  geom_function(fun = loglogistic, 
                args = list(a0 = 1, a1 = 0.2, b = 1, c = 3), 
                color = 'steelblue', linewidth = 1.25, alpha = 0.2)+
  geom_function(fun = expon, 
                args = list(a0 = 1, a1 = 0.2, b = 4.5), 
                color = 'sienna', linewidth = 1.25, alpha = 0.2)+
  geom_function(fun = hyperbolic,
                args = list(a0 = 1, a1 = 0.2, b = 2),
                color = 'olivedrab', linewidth = 1.25)+
  scale_x_continuous(name = 'X', limits = c(0, 20), breaks = 0:10*2)+
  scale_y_continuous(name = 'Y', limits = c(0, 1)) +
  
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = 1),
               arrow = grid::arrow(length = unit(5, 'mm')), 
               linetype = 2) +
  annotate('text', x = 1, y = 1, label = bquote('\u03b1'[0]), size = 5) +
  
  geom_hline(yintercept = 0.2, linetype = 2) +
  annotate('text', x = 20, y = 0.225, label = bquote('\u03b1'[1]), size = 5)
```

:::
:::

## Example 2: Herbicide Resistence in Pigweed{.smaller}

The "pigweed" experiment - originally from @fergusonALSInhibitorResistance2001 and republished by @bowleyHitchhikerGuideStatistics2015 - examined the response (as percent survival) exhibited by two biotypes of *Amaranthus powellii* S. Watson when exposed to increasing doses of the herbicide imazethapyr.

```{r}
#| eval: true
#| echo: true
#| output-location: 'column'

pigweed = data.frame(
   Biotype = factor(rep(LETTERS[1:2], each = 36)),
   Rate = c(rep(c(1, 4, 8, 16, 32,
                  64, 128, 256, 512),
                each = 4), 
            rep(c(0.001, 0.063, 0.125, 0.25,
                  0.5, 1, 2, 4, 16), 
                each = 4)), 
   Block = factor(rep(c(1:4), 18)), 
   Pct_Survival = c(100, 100, 100, 100, 100, NA, 
                     89.44, 61.53, 97.7, 100, 81.74,
                     61.04, 57.47, 100, 82.42, 73.71,
                     79.31, NA, 52.82, 69.51, 66.67,
                     65.44, 55.38, 51.56, 83.91,
                     76.5, 43.85, 42.18, 68.97, NA,
                     22.44, 34.62, 51.72, 60.14,
                     14.54, 15.85, NA, 100, 100, 100,
                     87.5, 74.3, 97.37, 42.87, 38.28,
                     50.37, 31.21, 27.17, 42.19, 38.68,
                     44.3, 45.87, 35.16, 19.65, 28.4,
                     27.35, 12.5, 14.31, 17.57, 39.44, 
                     7.81, 32.88, 17.63, 22.87, 8.59,
                     21.87, 19.36, 24.06, 11.72, 3.84, 
                     12.95, 19.89))

str(pigweed, width = 60, strict.width = 'cut')
```

## Exploratory Data Analysis (Ex. 2){.smaller}

```{r}
#| eval: true
#| echo: true
#| output-location: 'column'
ggplot(pigweed, aes(x = (Rate), y = Pct_Survival, color = Biotype))+
  # Plot the data itself
  facet_grid(cols = vars(Biotype), scales = 'free')+
  geom_point(size = 2) 
```

## Exploratory Data Analysis (Ex. 2){.smaller}

```{r}
#| echo: true
#| eval: true
#| output-location: 'column'
#| code-line-numbers: "|1-4|8-11"
# Define Log-Logistic function
loglogistic = function(x, ymax, ymin, slope, IC50){
  ymin + (ymax-ymin)/(1+exp(-b*(log(x)-log(IC50))))
}

ggplot(pigweed, aes(x = (Rate), y = Pct_Survival, color = Biotype))+
  # Plot the data itself
  facet_grid(cols = vars(Biotype), scales = 'free')+
  geom_point() +
  # Plot the log-logistic function
  geom_function(fun = loglogistic, 
                args = list(ymin = 30, ymax = 100, 
                            IC50 = 200, slope = -50))
```

## Model Development & Evaluation (Ex. 2){.smaller}

Fit the log-logistic model:

```{r}
#| echo: true
#| eval: true
#| code-line-numbers: "|1|3-4|5-7"
pigweed_mod_ll = gnls(Pct_Survival ~ ymin + (ymax - ymin)/(1+ exp(slope*(log(Rate) - log(EC50)))),
                      data = pigweed,
                      params = list(ymin + ymax ~ 1, 
                                    slope + EC50 ~ Biotype - 1),
                      start = list(ymin = 0, ymax = 100, 
                                   slope = c(.5, .5), 
                                   EC50 = c(1, 0.1)),
                      na.action = na.omit)
print(pigweed_mod_ll)
```
## Model Development & Evaluation (Ex. 2){.smaller}
Fit the hyperbolic model:

```{r}
#| echo: true
#| eval: true
#| code-line-numbers: "|1|3|4-6"
pigweed_mod_hy = gnls(Pct_Survival ~ ymin + slope*(y0 - ymin)/(slope+Rate),
                         data = pigweed,
                         params = list(ymin + y0 + slope ~ 0+Biotype),
                         start = list(ymin = c(20, 7), 
                                      y0 = c(100, 100), 
                                      slope = c(20, 0.25)),
                         na.action = na.omit)
print(pigweed_mod_hy)
```

## Model Development & Evaluation (Ex. 2){.smaller}
Fit the negative exponential model:

```{r}
#| echo: true
#| eval: true
#| code-line-numbers: "|1|3|4-6"
pigweed_mod_ne = gnls(Pct_Survival ~ ymin + (yinit - ymin)*exp(-Rate/slope),
                      data = pigweed,
                      params = list(yinit +  ymin + slope ~ 0+Biotype),
                      start = list(ymin = c(35, 15), 
                                   yinit = c(100, 100), 
                                   slope = c(70, 0.25)),
                      na.action = na.omit)
print(pigweed_mod_ne)
```
## Model Development & Evaluation (Ex. 2){.smaller}

AIC suggests that the log-logistic is the best fitting model, while visual check of the residuals does not suggest there are violations of the assumptions of normality and homoscedasticity.

:::columns
:::{.column width="50%"}
```{r}
#| echo: true
#| eval: true
AIC(pigweed_mod_ll, pigweed_mod_hy, pigweed_mod_ne)
```

<br/>

```{r}
#| echo: true
#| eval: false
pigweed_res = resid(pigweed_mod_ll, type = 'normalized')
pigweed_pred = fitted(pigweed_mod_ll)
resid_auxpanel(pigweed_res, pigweed_pred)
```
:::

:::{.column width="50%"}
```{r}
#| echo: false
#| eval: true
pigweed_res = resid(pigweed_mod_ll, type = 'normalized')
pigweed_pred = fitted(pigweed_mod_ll)
resid_auxpanel(pigweed_res, pigweed_pred)
```
:::
:::

## Estimation, Inference and Graphing (Ex. 2){.smaller}

All the tools from the `emmeans` package will work with models fit with `gnls`, you simply need to specify the parameter of interest via the argument `param`:

```{r}
#| echo: true
#| eval: true
#| output-location: 'column-fragment'
# Perform F-Test
joint_tests(pigweed_mod_ll, param = 'EC50')
```
<br/>

```{r}
#| echo: true
#| eval: true
#| output-location: 'column-fragment'
# Calculate estimated marginal means
(pigweed_emm_ll = emmeans(pigweed_mod_ll, ~ Biotype,
                          param = 'EC50'))
```
<br/>

```{r}
#| echo: true
#| eval: true
#| output-location: 'column-fragment'
# Perform contrasts, although with only
# two levels it is unneeded
contrast(pigweed_emm_ll, 'pairwise')
```

## Estimation, Inference and Graphing (Ex. 2){.smaller}

The `marginaleffects` package has additional tools for generating estimates and confidence band for the fitted trendline:

```{r}
#| echo: true
#| eval: true
#| fig-width: 10
#| fig-asp: 0.5
#| fig-align: 'center'
#| output-location: 'slide'
#| code-line-numbers: '|1-7|9-13|15-21'
# Generate a new data set containing all combinations of 
# Biotype and fine grid of Rate values. Note to remove 
# rows with biotype B and high rates or it messes up
# plots later
toplot = expand.grid(Biotype = factor(LETTERS[1:2]),
                     Rate = exp(seq(-6.9, 6.3, .1))) |> 
  filter(!(Biotype == 'B' & Rate > 17))

# Generate predictions for these new cultivar and density
# combinations
pigweed_preds = avg_predictions(model = pigweed_mod_ll, 
                                newdata = toplot, 
                                by = c('Biotype', 'Rate')) 

# Plot the results, along with the original data
ggplot(pigweed_preds, aes(x = Rate, color = Biotype, fill = Biotype)) +
  facet_grid(cols = vars(Biotype), scales = 'free')+
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high),
              color = NA, alpha = 0.2) +
  geom_line(aes(y = estimate))+
  geom_point(aes(y = Pct_Survival), data = pigweed, size = 2)
```


# Additional Resources{.smaller}
::: {layout-ncol=4}
![Chapter 3: *Deterministic Functions for Ecological Modeling* provides an excellent, software agnostic introduction to nonlinear models](bolker_cover.jpg){width=300} 

![This is the definative reference for the `nlme` package. Some chapters are highly mathematical, others very much applied. Most examples are not agriculture related, though.](bates_cover.jpg){width=300} 

![Chapter 15: *Nonlinear Regression Models and Applications* describes and details a large number the most commonly used nonlinear models in agricultural and related disciplines, and provides example code in R](glaz_cover.jpg){width=300} 

![Extended treatment of both mathematical/theorectical and practical aspects of nonlinear modeling in plant and soil sciences, but example code is almost all in SAS](schabenberger_cover.jpg){width=300}  

:::


## References